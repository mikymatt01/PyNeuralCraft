{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Monk Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "problem   = \"monks-3\"\n",
    "filename  = f\"monk/{problem}\"\n",
    "\n",
    "train       = \".train\"\n",
    "test        = \".test\"\n",
    "\n",
    "f = open(filename + train, 'r')\n",
    "res = f.readlines()\n",
    "f.close()\n",
    "train_str = ''.join(res)\n",
    "\n",
    "f = open(filename + test, 'r')\n",
    "res = f.readlines()\n",
    "f.close()\n",
    "test_str = ''.join(res)\n",
    "\n",
    "def retrieveData(data_str):\n",
    "    # Create a DataFrame from the structured data\n",
    "    encoding_length = []\n",
    "    column_names = [\"R\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"data\"]\n",
    "    column_features = [\"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\"]\n",
    "    data = pd.read_csv(StringIO(data_str), sep=' ', header=None, names=column_names)\n",
    "    data = data.iloc[:, :-1]\n",
    "    for col in column_features:\n",
    "        encoding_length.append(max(data[col].unique()))\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    \n",
    "    #scaler = MinMaxScaler()\n",
    "    #df_scaled = scaler.fit_transform(data.to_numpy())\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    del df_scaled['R']\n",
    "    #del df_scaled['Id']\n",
    "    df_scaled = df_scaled.assign(R=data['R'].values)\n",
    "    df_train = df_scaled\n",
    "    \n",
    "    features = 6\n",
    "    X_train = df_train.iloc[ : , :features].values\n",
    "    y_train = df_train.iloc[:,features:].values\n",
    "    return X_train, y_train, encoding_length\n",
    "\n",
    "def oneHotEncoding(X_data, l):\n",
    "    X_result = []\n",
    "    for x in X_data:\n",
    "        p = []\n",
    "        for i in range(len(x)):\n",
    "            d = [0] * l[i]\n",
    "            if x[i] == 1:\n",
    "                d[0] = 1\n",
    "            elif x[i] == 2:\n",
    "                d[1] = 1\n",
    "            elif x[i] == 3:\n",
    "                d[2] = 1\n",
    "            elif x[i] == 4:\n",
    "                d[3] = 1\n",
    "            p += d\n",
    "        X_result.append(p)\n",
    "    return X_result\n",
    "\n",
    "X_train, y_train, encoding_length = retrieveData(train_str)\n",
    "X_train = oneHotEncoding(X_train, encoding_length)\n",
    "\n",
    "X_test, y_test, encoding_length = retrieveData(test_str)\n",
    "X_test = oneHotEncoding(X_test, encoding_length)\n",
    "\n",
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import instantiate_act_func\n",
    "from layer import Layer\n",
    "from mlp import MLP\n",
    "from losses import instantiate_loss\n",
    "from grid_search import create_test\n",
    "from weigth_init import instantiate_initializer\n",
    "from utils import k_fold_cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_config = [\n",
    "    \"models/model1.json\", \n",
    "    \"models/model2.json\", \n",
    "]\n",
    "tests = create_test(json_file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_test(test):\n",
    "    layers = []\n",
    "    for layer in test['layers']:\n",
    "        layers.append(\n",
    "            Layer(\n",
    "                layer['units'],\n",
    "                instantiate_act_func(layer['act_func']),\n",
    "                layer['inputs'],\n",
    "                weights_initializer=instantiate_initializer(test['weights_initializer']),\n",
    "                kernel_regularizer=test['kernel_regularizer'],\n",
    "                bias_regularizer=test['bias_regularizer'],\n",
    "                momentum=test['momentum'],\n",
    "                Nesterov=test['Nesterov']\n",
    "            )\n",
    "        )\n",
    "    mlp = MLP(layers)\n",
    "    mlp.compile(test['learning_rate'],instantiate_loss(test['loss']), test['metrics'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path, test, accuracy, errors, summary):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    filename = f\"{path}/{iso_date}-acc:{str(round(accuracy, 2))}\"\n",
    "    f = open(f\"{filename}.logs\", 'w')\n",
    "    f.write(f\"{str(test)}\\n\")\n",
    "    f.write(f\"{summary}\\n\")\n",
    "    f.close()\n",
    "    plt.plot(errors)\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_selection_result = f\"results/model-selection/{problem}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "for test in tests:\n",
    "    epochs = round(test['epochs'] / k)\n",
    "    model = create_model_from_test(test)\n",
    "    dataset = k_fold_cross_validation(X_train, y_train, k)\n",
    "    print(test)\n",
    "    errors = []\n",
    "\n",
    "    for fold in dataset:\n",
    "        errors += model.fit(fold['X_train'], fold['y_train'], epochs)\n",
    "        _, accuracy = model.evaluate(fold['X_val'], fold['y_val'])\n",
    "        summary = model.summary()\n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = test\n",
    "\n",
    "    save_result(path_model_selection_result, test, accuracy, errors, summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model  Assessment - Hold-out Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_assessment_result = f\"results/model-assessment/{problem}\"\n",
    "best_model = {'learning_rate': 0.08, 'momentum': 0.8, 'Nesterov': True, 'kernel_regularizer': 0, 'bias_regularizer': 0, 'weights_initializer': 'random_init', 'layers': [{'units': 4, 'inputs': 17, 'act_func': 'relu'}, {'units': 1, 'inputs': 4, 'act_func': 'sigmoid'}], 'name': 'model3', 'epochs': 400, 'loss': 'mean_squared_error', 'metrics': ['accuracy']}\n",
    "model = create_model_from_test(best_model)\n",
    "\n",
    "errors = model.fit(X_train, y_train, best_model['epochs'])\n",
    "error, accuracy = model.evaluate(X_test, y_test)\n",
    "summary = model.summary()\n",
    "\n",
    "save_result(path_model_assessment_result, best_model, accuracy, errors, summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
